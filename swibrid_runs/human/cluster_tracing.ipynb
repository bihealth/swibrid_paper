{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe859ab-15ae-464a-b574-8c91e94a44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from swibrid.plot_clustering import *\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5423b719-a37e-4572-b974-56bcbfacacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Namespace(switch_coords='chr14:105583000-105872000:-',\n",
    "               switch_annotation='hg38_switch_regions.bed',\n",
    "               cutoff_color='r',\n",
    "               color_by='meta_cluster2',\n",
    "               chunksize=5000,\n",
    "               variants_matrix=None,\n",
    "               info='dummy',\n",
    "               cmax=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a926a481-4353-4d61-a251-129be43f68a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 251113 10:37:39 utils:53] using switch annotation from hg38_switch_regions.bed\n"
     ]
    }
   ],
   "source": [
    "    import matplotlib\n",
    "    import scipy.sparse\n",
    "    from matplotlib import pyplot as plt\n",
    "    from logzero import logger\n",
    "    from swibrid.utils import (\n",
    "        parse_switch_coords,\n",
    "        read_switch_anno,\n",
    "        get_switch_coverage,\n",
    "        shift_coord,\n",
    "    )\n",
    "\n",
    "    matplotlib.rcParams.update({\"font.size\": 8})\n",
    "    matplotlib.rcParams.update({\"axes.linewidth\": 0.5})\n",
    "    matplotlib.rcParams.update({\"xtick.major.width\": 0.5})\n",
    "    matplotlib.rcParams.update({\"ytick.major.width\": 0.5})\n",
    "\n",
    "    # increase recursion limit for plotting large dendrograms\n",
    "    sys.setrecursionlimit(100000)\n",
    "\n",
    "    scale_bar_x_length = 5000\n",
    "    scale_bar_x_legend = \"5kb\"\n",
    "\n",
    "    (\n",
    "        switch_chrom,\n",
    "        switch_start,\n",
    "        switch_end,\n",
    "        switch_orientation,\n",
    "    ) = parse_switch_coords(args.switch_coords)\n",
    "    switch_anno = read_switch_anno(args.switch_annotation)\n",
    "    cov_int, Ltot, eff_start, eff_end, anno_recs = get_switch_coverage(\n",
    "        switch_anno, switch_chrom, switch_start, switch_end\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78418170-5b09-4306-b5ec-8eadd920b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot_main_image(args, msa, ax, Ltot, reads, clustering, order, values, cmap, variants=None, vmat=None):\n",
    "    \"\"\"plot the image in chunks\"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import re\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    nreads = len(reads)\n",
    "\n",
    "    nchunks = np.ceil(nreads / args.chunksize).astype(int)\n",
    "    for n in range(nchunks):\n",
    "        order_chunk = order[n * args.chunksize : min((n + 1) * args.chunksize, nreads)]\n",
    "        extent = [\n",
    "            0,\n",
    "            Ltot,\n",
    "            nreads - min((n + 1) * args.chunksize, nreads),\n",
    "            nreads - n * args.chunksize,\n",
    "        ]\n",
    "        msa_chunk = msa[order_chunk]\n",
    "        im = np.empty(msa_chunk.shape)\n",
    "        im[:] = np.nan\n",
    "        tmp = np.broadcast_to(values[order_chunk], msa_chunk.T.shape).T\n",
    "        im[np.nonzero(msa_chunk)] = tmp[np.nonzero(msa_chunk)]\n",
    "\n",
    "        ax.imshow(\n",
    "            im,\n",
    "            aspect=\"auto\",\n",
    "            interpolation=\"nearest\",\n",
    "            cmap=cmap,\n",
    "            extent=extent,\n",
    "            vmin=0,\n",
    "            vmax=19\n",
    "        ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a743f89c-3669-4fd6-94f6-1295e375d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot_inserts(args, cov_int, Ltot, ax, lw, nreads, order, clustering, switch_orientation):\n",
    "    \"\"\"plot insert locations\"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    from matplotlib.collections import LineCollection\n",
    "    from swibrid.utils import (\n",
    "        decode_insert,\n",
    "        interval_length,\n",
    "        intersect_intervals,\n",
    "        merge_intervals,\n",
    "        shift_coord,\n",
    "    )\n",
    "\n",
    "    assert args.coords is not None, \"need processed read coordinates to show inserts!\"\n",
    "\n",
    "    process = pd.read_csv(args.coords, sep=\"\\t\", header=0, index_col=0)\n",
    "    read_inserts = dict(\n",
    "        (read, [decode_insert(insert) for insert in inserts.split(\";\")])\n",
    "        for read, inserts in process[\"inserts\"].dropna().items()\n",
    "    )\n",
    "\n",
    "    inserts = [\n",
    "        (\n",
    "            m.group(\"insert_chrom\"),\n",
    "            int(m.group(\"insert_start\")),\n",
    "            int(m.group(\"insert_end\")),\n",
    "        )\n",
    "        for read, inserts in read_inserts.items()\n",
    "        for m in inserts\n",
    "    ]\n",
    "    ninserts = len(inserts)\n",
    "    unique_inserts = merge_intervals(inserts)\n",
    "    ninserts_unique = len(unique_inserts)\n",
    "    nclusts = len(np.unique(clustering[\"cluster\"].astype(int)))\n",
    "    nclusts_eff = len(\n",
    "        np.unique(clustering[\"filtered_cluster\"][clustering[\"filtered_cluster\"] >= 0])\n",
    "    )\n",
    "\n",
    "    dx = 0.01 * Ltot\n",
    "\n",
    "    for read in clustering.iloc[order].dropna(subset=[\"inserts\"]).index:\n",
    "        if read not in read_inserts:\n",
    "            continue\n",
    "        p = nreads - clustering.index[order].get_loc(read) - 1\n",
    "        for m in read_inserts[read]:\n",
    "            insert_chrom = m.group(\"insert_chrom\")\n",
    "            insert_start = int(m.group(\"insert_start\"))\n",
    "            insert_end = int(m.group(\"insert_end\"))\n",
    "            switch_left = int(m.group(\"switch_left\"))\n",
    "            switch_right = int(m.group(\"switch_right\"))\n",
    "            if (switch_orientation == \"+\" and switch_right < switch_left) or (\n",
    "                switch_orientation == \"-\" and switch_right > switch_left\n",
    "            ):\n",
    "                switch_right, switch_left = switch_left, switch_right\n",
    "            insert = (insert_chrom, insert_start, insert_end)\n",
    "            ax.plot(\n",
    "                shift_coord(switch_left, cov_int),\n",
    "                p + 0.5,\n",
    "                \">\",\n",
    "                color=\"k\",\n",
    "                markersize=0.5,\n",
    "            )\n",
    "            ax.plot(\n",
    "                shift_coord(switch_right, cov_int),\n",
    "                p + 0.5,\n",
    "                \"<\",\n",
    "                color=\"k\",\n",
    "                markersize=0.5,\n",
    "            )\n",
    "            \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7190ebf6-cf29-4a3c-980a-e3c20c621bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot_read_alignments(args, cov_int, Ltot, ax, fig, lw, nreads, mark_reads, mark_labels, mark_colors, order, clustering, realignments, switch_orientation):\n",
    "    \"\"\"plot insert locations\"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    from matplotlib.collections import LineCollection\n",
    "    #from adjustText import adjust_text    \n",
    "    from swibrid.utils import (\n",
    "        decode_insert,\n",
    "        interval_length,\n",
    "        intersect_intervals,\n",
    "        merge_intervals,\n",
    "        shift_coord,\n",
    "    )\n",
    "\n",
    "    nlocs = len(mark_reads)\n",
    "    dx = 0.01 * Ltot\n",
    "\n",
    "    mark_pos = {}\n",
    "\n",
    "    arrows = []\n",
    "    texts = []\n",
    "    all_labels = []\n",
    "    for k, (read,label) in enumerate(zip(mark_reads,mark_labels)):\n",
    "        p = nreads - clustering.index[order].get_loc(read) - 1\n",
    "        if switch_orientation == \"+\":\n",
    "            arrows.append([(Ltot, p + 0.5), (Ltot + dx, p + 0.5)])          \n",
    "        else:\n",
    "            arrows.append([(0, p + 0.5), (-dx, p + 0.5)])\n",
    "\n",
    "        labels = [f\"{label} {read}\"]\n",
    "        for _,x in realignments.loc[[read]].sort_values(\"pos_left\", ascending=False).iterrows():\n",
    "            if x['type']=='switch':\n",
    "                labels.append('chr14:{3}-{4}: n_homology={0}, n_untemplated={1}'.format(x['n_homology'],x['n_untemplated'], k+1, \n",
    "                                                                                        x['pos_left'].split(':')[1],\n",
    "                                                                                        x['pos_right'].split(':')[1]))\n",
    "            else:\n",
    "                labels.append('{3}-{4}'.format(x['n_homology'],x['n_untemplated'], k+1, \n",
    "                                               x['pos_left'],\n",
    "                                               x['pos_right']))\n",
    "            labels.append('\\n'.join(x[['left_seq', 'match_left', 'read_seq', 'match_right','right_seq']]).upper())\n",
    "\n",
    "        texts.append(ax.text(\n",
    "            -4.2 * dx,\n",
    "            p + 0.5,\n",
    "            '\\n'.join(labels),\n",
    "            size=\"xx-small\",\n",
    "            color=\"k\",\n",
    "            clip_on=False,\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            family='monospace',\n",
    "            bbox=dict(facecolor='none', edgecolor=mark_colors[k], pad=1.0, linewidth=1)\n",
    "        ))\n",
    "\n",
    "        ax.add_patch(matplotlib.patches.Rectangle((0, p + .5 -10), Ltot, 20, linewidth=lw, edgecolor='gray', facecolor='none'))\n",
    "\n",
    "        all_labels.append(labels)\n",
    "\n",
    "\n",
    "    transf = ax.transData.inverted()\n",
    "    text_coords=[t.get_window_extent(renderer = fig.canvas.get_renderer()).transformed(transf).corners() for t in texts]\n",
    "    text_heights=[tc[1][1]-tc[0][1] for tc in text_coords]\n",
    "    sp=(np.diff(ax.get_ylim())[0]-sum(text_heights))/len(text_heights)\n",
    "    assert sp > 0, 'negative space when adjusting labels'\n",
    "        \n",
    "    arrows=[]\n",
    "    newp=ax.get_ylim()[1] \n",
    "    for k, t in enumerate(texts):\n",
    "        x, p = t.get_position()\n",
    "        newp -= .5*text_heights[k]\n",
    "        t.set_position((x, newp))\n",
    "        arrows.append([(-dx, p), (-3 * dx, newp)])\n",
    "        arrows.append([(-3 * dx, newp), (-4 * dx, newp)])\n",
    "        newp -= .5*text_heights[k] + sp \n",
    "\n",
    "    ax.add_collection(LineCollection(arrows, linewidths=0.25, colors=\"k\", clip_on=False))\n",
    "\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16feb066-0c24-4cdd-a785-d4615a837e08",
   "metadata": {},
   "source": [
    "# triplicates with 50000 cells of donor 21084 (=A in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "268d34e8-b390-44de-a20d-7bce8eb6fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "donor='21084'\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dcbd7133-d161-4559-bde2-286ccbb7eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_clustering=pd.read_csv(f'meta_clustering/{donor}_50000_meta_clustering.csv',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cc64111-c55a-490a-b7b5-f07c83ef5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=meta_clustering.groupby('meta_cluster').apply(lambda df: pd.Series({'nreads_tot': df.shape[0], 'n_samples': len(df['sample'].unique()), \n",
    "                                                                       'min_reads_per_sample': df.groupby('sample').size().min(),\n",
    "                                                                      'samples':','.join(df['sample'].unique())})).sort_values('nreads_tot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cab33268-3228-4631-b033-799ccf0ae716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nreads_tot</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>min_reads_per_sample</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>213</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>213</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>214</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>217</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>224</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>251</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>259</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>261</td>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>295</td>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>361</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>612</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>669</td>\n",
       "      <td>3</td>\n",
       "      <td>196</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>874</td>\n",
       "      <td>3</td>\n",
       "      <td>235</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nreads_tot  n_samples  min_reads_per_sample   \n",
       "meta_cluster                                                \n",
       "458                  213          3                    36  \\\n",
       "1126                 213          3                    50   \n",
       "2137                 214          3                    56   \n",
       "267                  217          3                    49   \n",
       "296                  224          3                    65   \n",
       "559                  234          3                    59   \n",
       "176                  242          3                    47   \n",
       "399                  251          3                    50   \n",
       "450                  259          3                    60   \n",
       "1520                 261          3                    77   \n",
       "222                  295          3                    77   \n",
       "243                  361          3                    92   \n",
       "79                   612          3                   160   \n",
       "80                   669          3                   196   \n",
       "274                  874          3                   235   \n",
       "\n",
       "                                                        samples  \n",
       "meta_cluster                                                     \n",
       "458           20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "1126          20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "2137          20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "267           20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "296           20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "559           20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "176           20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "399           20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "450           20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "1520          20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "222           20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "243           20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "79            20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "80            20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "274           20211102_84_n50000_1,20211102_84_n50000_2,2021...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_clusters=df[df['n_samples']==3].iloc[-15:]\n",
    "top_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7abc0ab4-35e4-450c-911e-67c11c73f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=top_clusters['samples'].unique()[0].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c18a99f-581f-4546-800f-3c12ae209ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = dict((sample,pd.read_csv(f'pipeline/{sample}/{sample}_clustering.csv', index_col=0, header=0)) for sample in samples)\n",
    "realignments = dict((sample,pd.read_csv(f'pipeline/{sample}/{sample}_breakpoint_alignments.csv',header=0,index_col=0)) for sample in samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f8d7da0-2742-49d6-b8f6-55fbee80e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples:\n",
    "    realignments[sample][\"chrom_left\"] = realignments[sample][\"pos_left\"].str.split(\":\").str[0]\n",
    "    realignments[sample][\"chrom_right\"] = realignments[sample][\"pos_right\"].str.split(\":\").str[0]\n",
    "    realignments[sample][\"pleft\"] = realignments[sample][\"pos_left\"].str.split(\":\").str[1].astype(int)\n",
    "    realignments[sample][\"pright\"] = realignments[sample][\"pos_right\"].str.split(\":\").str[1].astype(int)\n",
    "    \n",
    "    # filter out realignments across breaks smaller than max_realignment_gap      \n",
    "    gap_size = np.abs(realignments[sample][\"pleft\"] - realignments[sample][\"pright\"])\n",
    "    keep = (gap_size >= 75) | (realignments[sample][\"chrom_left\"] != realignments[sample][\"chrom_right\"])\n",
    "    realignments[sample] = realignments[sample][keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89fd26d6-5d04-470e-bc0b-e85ff56402e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = pd.concat([pd.read_csv(f'pipeline/{sample}/{sample}_processed.out', index_col=0, header=0, sep='\\t').loc[clustering[sample].index].dropna() for sample in samples], axis=0, keys=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "147f6f67-cb07-494c-acf4-1677aa8d6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swibrid.utils import (\n",
    "        decode_insert,\n",
    "        interval_length,\n",
    "        intersect_intervals,\n",
    "        merge_intervals,\n",
    "        shift_coord,\n",
    "    )\n",
    "read_inserts = dict((read, [decode_insert(insert) for insert in inserts.split(\";\")])\n",
    "        for read, inserts in coords[\"inserts\"].dropna().items())\n",
    "inserts = [(m.group(\"insert_chrom\"),int(m.group(\"insert_start\")), int(m.group(\"insert_end\")))\n",
    "           for read, inserts in read_inserts.items()for m in inserts ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb6ebb26-903c-4d0a-a5b3-3cd8b3c8f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_inserts = merge_intervals(inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "597491bf-f098-4ccd-829a-f1647fe2cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords['uinsert']=[','.join(['{0}:{1}-{2}'.format(*ins) for ins in unique_inserts if \n",
    "                             interval_length(intersect_intervals([(m.group(\"insert_chrom\"),\n",
    "                                                                   int(m.group(\"insert_start\")), \n",
    "                                                                   int(m.group(\"insert_end\")))], [ins])) > 0][0] for m in inserts) for read, inserts in read_inserts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d182a0f-75d9-42d5-b55c-40e6480c27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=coords.reset_index(names=['sample','read']).groupby('uinsert').apply(lambda df: pd.Series({'nreads_tot': df.shape[0], 'n_samples': len(df['sample'].unique()), \n",
    "                                                                       'min_reads_per_sample': df.groupby('sample').size().min(),\n",
    "                                                                      'samples':','.join(df['sample'].unique())})).sort_values('nreads_tot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0603d0c8-aab1-4260-ba90-734627e4e91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nreads_tot</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>min_reads_per_sample</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uinsert</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chr16:22336035-22336493</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr12:92467074-92467472</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr3:141391307-141391556</th>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          nreads_tot  n_samples  min_reads_per_sample   \n",
       "uinsert                                                                 \n",
       "chr16:22336035-22336493            6          3                     1  \\\n",
       "chr12:92467074-92467472            7          3                     2   \n",
       "chr3:141391307-141391556          21          3                     2   \n",
       "\n",
       "                                                                    samples  \n",
       "uinsert                                                                      \n",
       "chr16:22336035-22336493   20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "chr12:92467074-92467472   20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "chr3:141391307-141391556  20211102_84_n50000_1,20211102_84_n50000_2,2021...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_inserts=df2[df2['n_samples'] == 3].iloc[-10:]\n",
    "top_inserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e9fbc4d-17bc-4f67-825c-5ad473dc3eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nreads_tot</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>min_reads_per_sample</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10317</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20211102_84_n50000_1,20211102_84_n50000_2,2021...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nreads_tot  n_samples  min_reads_per_sample   \n",
       "meta_cluster                                                \n",
       "10317                  6          3                     1  \\\n",
       "659                   11          3                     2   \n",
       "\n",
       "                                                        samples  \n",
       "meta_cluster                                                     \n",
       "10317         20211102_84_n50000_1,20211102_84_n50000_2,2021...  \n",
       "659           20211102_84_n50000_1,20211102_84_n50000_2,2021...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=meta_clustering.loc[coords[coords['uinsert'].isin(top_inserts.index)].index.get_level_values(1)][['meta_cluster','sample']].groupby('meta_cluster').apply(lambda df: pd.Series({'nreads_tot': df.shape[0], 'n_samples': len(df['sample'].unique()), \n",
    "                                                                       'min_reads_per_sample': df.groupby('sample').size().min(),\n",
    "                                                                      'samples':','.join(df['sample'].unique())})).sort_values('nreads_tot')\n",
    "df3[df3['n_samples']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fcb26d45-4db8-4f1a-843c-ad2bedc3c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_insert_reads=meta_clustering.index[meta_clustering['meta_cluster'].isin([10317,659])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "375540c2-d49b-4350-9e71-b0bd1ce6dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_insert_reads={}\n",
    "for sample in samples:\n",
    "    selected_insert_reads[sample]=coords.loc[sample][coords.loc[sample]['uinsert'].isin(top_inserts.index) & coords.loc[sample].index.isin(top_insert_reads)][\"uinsert\"].drop_duplicates().index\n",
    "    coords.loc[sample].loc[selected_insert_reads[sample]].to_csv(f'meta_clustering/{sample}_processed_filtered.out', sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0d6f710-addd-46a2-90d7-c3a9225385d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clusters=list(top_clusters.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5cce3a45-2d81-4099-97da-984c676d9566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x1300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x1300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x1300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figs=[plt.figure(figsize=(9,13)) for sample in samples]\n",
    "lw = figs[0].bbox_inches.height * 72 / 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b41ee752-173a-4cc9-b538-f1ee623e22b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "13f12297-0919-46a3-9502-b688803018bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cephfs-1/home/users/obermayb_c/scratch/tmp/ipykernel_2428114/465303758.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ri[~ri['meta_cluster'].isin(selected_clusters)]['meta_cluster']=np.nan\n",
      "/data/cephfs-1/home/users/obermayb_c/scratch/tmp/ipykernel_2428114/465303758.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ri[~ri['meta_cluster'].isin(selected_clusters)]['meta_cluster']=np.nan\n",
      "/data/cephfs-1/home/users/obermayb_c/scratch/tmp/ipykernel_2428114/465303758.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ri[~ri['meta_cluster'].isin(selected_clusters)]['meta_cluster']=np.nan\n"
     ]
    }
   ],
   "source": [
    "cmap=plt.cm.tab20b\n",
    "cmap.set_under('#EEEEEE')\n",
    "cat_type = pd.CategoricalDtype(categories=np.arange(len(selected_clusters)).astype(str), ordered=True)\n",
    "cluster_map=dict(zip(np.array(selected_clusters).astype(str),np.arange(len(selected_clusters)).astype(str)))\n",
    "order={}\n",
    "\n",
    "for n, sample in enumerate(samples):\n",
    "\n",
    "    ax=figs[n].add_axes([.005,.01,.015,.975])\n",
    "    args.linkage=f'pipeline/{sample}/{sample}_linkage.npz'\n",
    "    order[sample]=plot_linkage(args, clustering[sample], clustering[sample]['cluster'].dropna().shape[0], ax, .1, lw)\n",
    "\n",
    "    ax=figs[n].add_axes([.02,.01,.4,.975])\n",
    "    msa=scipy.sparse.load_npz(f'pipeline/{sample}/{sample}_msa.npz').tocsr()\n",
    "    ri=meta_clustering[meta_clustering['sample']==sample].loc[clustering[sample].index]\n",
    "    ri[~ri['meta_cluster'].isin(selected_clusters)]['meta_cluster']=np.nan\n",
    "    ri['meta_cluster2']=ri['meta_cluster'].astype(str).map(cluster_map).astype(str).astype(cat_type)\n",
    "    values=ri['meta_cluster2'].cat.codes.values\n",
    "\n",
    "    variants = pd.read_csv(f'pipeline/{sample}/{sample}_variants.txt', sep=\"\\t\", header=0)\n",
    "    vmat = scipy.sparse.load_npz(f'pipeline/{sample}/{sample}_variants.npz').tocsr()\n",
    "\n",
    "    my_plot_main_image(args, msa, ax, Ltot, clustering[sample]['cluster'].dropna().index, \n",
    "                       clustering[sample], order[sample], values, cmap, variants, vmat)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    xlim = [0, Ltot]\n",
    "    ylim = [0, msa.shape[0]]\n",
    "    ax.set_xlim(xlim if switch_orientation == \"+\" else xlim[::-1])\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    major_ticks = []\n",
    "    minor_ticks = []\n",
    "    minor_labels = []\n",
    "    for rec in anno_recs:\n",
    "        start = shift_coord(int(rec[3][1]), cov_int)\n",
    "        end = shift_coord(int(rec[3][2]), cov_int)\n",
    "        major_ticks += [start, end]\n",
    "        minor_ticks.append((start + end) / 2)\n",
    "        minor_labels.append(rec[3][3])\n",
    "    ax.set_xticks(np.unique(major_ticks))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_xticklabels(minor_labels, minor=True, size=\"small\")\n",
    "    ax.tick_params(which=\"minor\", length=0)\n",
    "    \n",
    "    args.coords=f'meta_clustering/{sample}_processed_filtered.out'\n",
    "    my_plot_inserts(args, cov_int, Ltot, ax, lw, msa.shape[0], order[sample], clustering[sample], switch_orientation)\n",
    "\n",
    "    selected_reads=np.array([np.random.choice(meta_clustering[(meta_clustering['meta_cluster']==c) & (meta_clustering['sample']==sample)].index, size=1)[0] for c in top_clusters.index] + list(selected_insert_reads[sample]))\n",
    "    selected_labels=np.array([f'#{k+1}' for k in range(len(selected_reads))])\n",
    "    selected_colors=np.array([cmap(int(cluster_map[str(c)])) for c in selected_clusters] + [(0,0,0,1) for i in selected_insert_reads[sample]])\n",
    "    selected_order=pd.Series(dict((read,clustering[sample].index[order[sample]].get_loc(read)) for read in selected_reads)).argsort()\n",
    "    label_text[sample] = my_plot_read_alignments(args, cov_int, Ltot, ax, figs[n], lw, msa.shape[0], selected_reads[selected_order], selected_labels[selected_order],\n",
    "                            selected_colors[selected_order], order[sample], clustering[sample], realignments[sample], switch_orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a96b0310-dfaa-4cd0-b849-8e39181ca259",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,sample in enumerate(samples):\n",
    "    replicate=n+1\n",
    "    figs[n].text(0.01, 0.99, f'top recurring clusters in replicate {replicate} of 50000 B cells of donor A', \n",
    "                 size=\"small\", ha=\"left\", va=\"bottom\")\n",
    "    figs[n].savefig(f'meta_clustering/{sample}_cluster_tracing.pdf' , dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7160950-cb8d-42bb-9cff-1fa4f0f871e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c35fb-6f9a-4dee-ac35-ac01759d1910",
   "metadata": {},
   "source": [
    "# comparison of pacBio and minION reads from the same sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ef946de-b209-4263-ad8d-b9642fc83755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nreads_tot</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>min_reads_per_sample</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>124</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>138</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>139</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>143</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>157</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>162</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>179</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>232</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>484</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>20230313_HD19005,20230504_HD19005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nreads_tot  n_samples  min_reads_per_sample   \n",
       "meta_cluster                                                \n",
       "22                   124          2                    31  \\\n",
       "0                    125          2                    49   \n",
       "143                  125          2                    14   \n",
       "120                  125          2                    37   \n",
       "11                   127          2                    40   \n",
       "19                   132          2                    45   \n",
       "48                   138          2                    42   \n",
       "39                   139          2                    19   \n",
       "82                   143          2                    40   \n",
       "112                  144          2                    42   \n",
       "79                   150          2                    34   \n",
       "171                  157          2                    45   \n",
       "7                    162          2                    57   \n",
       "31                   171          2                    57   \n",
       "95                   171          2                    64   \n",
       "75                   179          2                    51   \n",
       "33                   180          2                    69   \n",
       "46                   190          2                    73   \n",
       "42                   232          2                    64   \n",
       "1                    484          2                   173   \n",
       "\n",
       "                                        samples  \n",
       "meta_cluster                                     \n",
       "22            20230313_HD19005,20230504_HD19005  \n",
       "0             20230313_HD19005,20230504_HD19005  \n",
       "143           20230313_HD19005,20230504_HD19005  \n",
       "120           20230313_HD19005,20230504_HD19005  \n",
       "11            20230313_HD19005,20230504_HD19005  \n",
       "19            20230313_HD19005,20230504_HD19005  \n",
       "48            20230313_HD19005,20230504_HD19005  \n",
       "39            20230313_HD19005,20230504_HD19005  \n",
       "82            20230313_HD19005,20230504_HD19005  \n",
       "112           20230313_HD19005,20230504_HD19005  \n",
       "79            20230313_HD19005,20230504_HD19005  \n",
       "171           20230313_HD19005,20230504_HD19005  \n",
       "7             20230313_HD19005,20230504_HD19005  \n",
       "31            20230313_HD19005,20230504_HD19005  \n",
       "95            20230313_HD19005,20230504_HD19005  \n",
       "75            20230313_HD19005,20230504_HD19005  \n",
       "33            20230313_HD19005,20230504_HD19005  \n",
       "46            20230313_HD19005,20230504_HD19005  \n",
       "42            20230313_HD19005,20230504_HD19005  \n",
       "1             20230313_HD19005,20230504_HD19005  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donor='HD19005'\n",
    "meta_clustering=pd.read_csv(f'meta_clustering/{donor}_meta_clustering.csv',header=0,index_col=0)\n",
    "df=meta_clustering.groupby('meta_cluster').apply(lambda df: pd.Series({'nreads_tot': df.shape[0], 'n_samples': len(df['sample'].unique()), \n",
    "                                                                       'min_reads_per_sample': df.groupby('sample').size().min(),\n",
    "                                                                      'samples':','.join(df['sample'].unique())})).sort_values('nreads_tot')\n",
    "top_clusters=df[df['n_samples']==2].iloc[-20:]\n",
    "samples=top_clusters['samples'].unique()[0].split(\",\")\n",
    "top_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f6413a3-7dd0-4ccd-98e7-96d539a67642",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clusters=list(top_clusters.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d2a06ca6-614b-4585-9287-6ff518053a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = dict((sample,pd.read_csv(f'pipeline/{sample}/{sample}_clustering.csv', index_col=0, header=0)) for sample in samples)\n",
    "realignments = dict((sample,pd.read_csv(f'pipeline/{sample}/{sample}_breakpoint_alignments.csv',header=0,index_col=0)) for sample in samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c4801342-b365-4280-859a-cff251e58c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples:\n",
    "    realignments[sample][\"chrom_left\"] = realignments[sample][\"pos_left\"].str.split(\":\").str[0]\n",
    "    realignments[sample][\"chrom_right\"] = realignments[sample][\"pos_right\"].str.split(\":\").str[0]\n",
    "    realignments[sample][\"pleft\"] = realignments[sample][\"pos_left\"].str.split(\":\").str[1].astype(int)\n",
    "    realignments[sample][\"pright\"] = realignments[sample][\"pos_right\"].str.split(\":\").str[1].astype(int)\n",
    "    \n",
    "    # filter out realignments across breaks smaller than max_realignment_gap      \n",
    "    gap_size = np.abs(realignments[sample][\"pleft\"] - realignments[sample][\"pright\"])\n",
    "    keep = (gap_size >= 75) | (realignments[sample][\"chrom_left\"] != realignments[sample][\"chrom_right\"])\n",
    "    realignments[sample] = realignments[sample][keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa744ef4-8e8a-4070-a3ce-265048aadb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x1300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x1300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figs=[plt.figure(figsize=(9,13)) for sample in samples]\n",
    "lw = figs[0].bbox_inches.height * 72 / 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d37b9cc-f979-42ee-a864-54f3649537e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cephfs-1/home/users/obermayb_c/scratch/tmp/ipykernel_2428114/2912412467.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ri[~ri['meta_cluster'].isin(selected_clusters)]['meta_cluster']=np.nan\n",
      "/data/cephfs-1/home/users/obermayb_c/scratch/tmp/ipykernel_2428114/2912412467.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ri[~ri['meta_cluster'].isin(selected_clusters)]['meta_cluster']=np.nan\n"
     ]
    }
   ],
   "source": [
    "cmap=plt.cm.tab20b\n",
    "cmap.set_under('#EEEEEE')\n",
    "cat_type = pd.CategoricalDtype(categories=np.arange(len(selected_clusters)).astype(str), ordered=True)\n",
    "cluster_map=dict(zip(np.array(selected_clusters).astype(str),np.arange(len(selected_clusters)).astype(str)))\n",
    "order={}\n",
    "\n",
    "for n, sample in enumerate(samples):\n",
    "\n",
    "    ax=figs[n].add_axes([.005,.01,.015,.975])\n",
    "    args.linkage=f'pipeline/{sample}/{sample}_linkage.npz'\n",
    "    order[sample]=plot_linkage(args, clustering[sample], clustering[sample]['cluster'].dropna().shape[0], ax, .1, lw)\n",
    "\n",
    "    ax=figs[n].add_axes([.02,.01,.6,.975])\n",
    "    msa=scipy.sparse.load_npz(f'pipeline/{sample}/{sample}_msa.npz').tocsr()\n",
    "    ri=meta_clustering[meta_clustering['sample']==sample].loc[clustering[sample].index]\n",
    "    ri[~ri['meta_cluster'].isin(selected_clusters)]['meta_cluster']=np.nan\n",
    "    ri['meta_cluster2']=ri['meta_cluster'].astype(str).map(cluster_map).astype(str).astype(cat_type)\n",
    "    values=ri['meta_cluster2'].cat.codes.values\n",
    "\n",
    "    variants = pd.read_csv(f'pipeline/{sample}/{sample}_variants.txt', sep=\"\\t\", header=0)\n",
    "    vmat = scipy.sparse.load_npz(f'pipeline/{sample}/{sample}_variants.npz').tocsr()\n",
    "\n",
    "    my_plot_main_image(args, msa, ax, Ltot, clustering[sample]['cluster'].dropna().index, \n",
    "                       clustering[sample], order[sample], values, cmap, variants, vmat)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    xlim = [0, Ltot]\n",
    "    ylim = [0, msa.shape[0]]\n",
    "    ax.set_xlim(xlim if switch_orientation == \"+\" else xlim[::-1])\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    major_ticks = []\n",
    "    minor_ticks = []\n",
    "    minor_labels = []\n",
    "    for rec in anno_recs:\n",
    "        start = shift_coord(int(rec[3][1]), cov_int)\n",
    "        end = shift_coord(int(rec[3][2]), cov_int)\n",
    "        major_ticks += [start, end]\n",
    "        minor_ticks.append((start + end) / 2)\n",
    "        minor_labels.append(rec[3][3])\n",
    "    ax.set_xticks(np.unique(major_ticks))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_xticklabels(minor_labels, minor=True, size=\"small\")\n",
    "    ax.tick_params(which=\"minor\", length=0)\n",
    "    \n",
    "    selected_reads=np.array([np.random.choice(meta_clustering[(meta_clustering['meta_cluster']==c) & (meta_clustering['sample']==sample)].index, size=1)[0] for c in top_clusters.index])\n",
    "    selected_labels=np.array([f'#{k+1}' for k in range(top_clusters.shape[0])])\n",
    "    selected_colors=np.array([cmap(int(cluster_map[str(c)])) for c in top_clusters.index])\n",
    "    selected_order=pd.Series(dict((read,clustering[sample].index[order[sample]].get_loc(read)) for read in selected_reads)).argsort()\n",
    "    label_text[sample]=my_plot_read_alignments(args, cov_int, Ltot, ax, figs[n], lw, msa.shape[0], selected_reads[selected_order], \n",
    "                            selected_labels[selected_order], selected_colors[selected_order], order[sample], clustering[sample], realignments[sample], switch_orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c7f8943-448b-4ed9-bb44-d1ad0ad47687",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,sample in enumerate(samples):\n",
    "    method='minION' if sample.startswith('20230313') else 'pacBio'\n",
    "    figs[n].text(0.01, 0.99, f'top recurring clusters for PBMCs ({method} sequencing)', \n",
    "                 size=\"small\", ha=\"left\", va=\"bottom\")\n",
    "    figs[n].savefig(f'meta_clustering/{sample}_cluster_tracing.pdf' , dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3bd4540-011e-4403-a085-ee5d2e8d6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9be7bb62-7afe-4cd0-83e8-2db04be51035",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"cluster_tracing.xlsx\") as writer:\n",
    "    for n,sample in enumerate(['20211102_84_n50000_1', '20211102_84_n50000_2', '20211122_84_n50000_3']):\n",
    "        replicate=n+1\n",
    "        sheet=f'B cells of donor A (R{replicate})'\n",
    "        pd.DataFrame(label_text[sample]).to_excel(writer, sheet_name=sheet, header=False, index=False)\n",
    "    for n,sample in enumerate(['20230313_HD19005', '20230504_HD19005']):\n",
    "        method='minION' if sample.startswith('20230313') else 'pacBio'\n",
    "        sheet=f'{method} sequencing of PBMCs'\n",
    "        pd.DataFrame(label_text[sample]).to_excel(writer, sheet_name=sheet, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b1eae-9766-4dcb-8c58-1afb3465147c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
